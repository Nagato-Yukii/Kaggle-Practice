{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f5b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f49434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data ---\n",
      "--- Engineering 'match_p' feature using merge ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Data ---\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "datasert_df = pd.read_csv('personality_dataset.csv')#已经用mean填充了数值,mode填充了分类型数据\n",
    "\n",
    "print(\"--- Engineering 'match_p' feature using merge ---\")\n",
    "\n",
    "datasert_df_prep = datasert_df.rename(columns={'Personality': 'match_p'}).drop_duplicates(['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance','Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency'])\n",
    "merge_cols = ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance','Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency']\n",
    "train_df = train_df.merge(datasert_df_prep, how='left', on=merge_cols)\n",
    "test_df = test_df.merge(datasert_df_prep, how='left', on=merge_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7fcdc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Performing Imputation and Initial Feature Prep ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Performing Imputation and Initial Feature Prep ---\")\n",
    "train_id = train_df['id']; test_id = test_df['id']\n",
    "y_train_series = train_df['Personality']\n",
    "all_data = pd.concat([train_df.drop(['id', 'Personality'], axis=1), test_df.drop('id', axis=1)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "288e9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_by_quantile_group(df, group_source_col, target_col):\n",
    "    \"\"\"\n",
    "    按分位数分组，使用组内中位数填充目标列的缺失值。\n",
    "\n",
    "    该函数实现了一种智能的缺失值填充方法。它首先根据 `group_source_col` 列的\n",
    "    数值分布(分位数),将数据分成四组(0-25%, 25-50%, 50-75%, 75-100%).然后，\n",
    "    计算每个组内 `target_col` 列的中位数。最后，使用各行对应分组的中位数来填充\n",
    "    `target_col` 中的缺失值(NaN)。\n",
    "\n",
    "    这种方法比使用全局中位数填充更精细，特别适用于源列和目标列之间存在相关性的场景\n",
    "    （例如，使用年龄分组来填充收入的缺失值）。\n",
    "\n",
    "    参数:\n",
    "        df (pd.DataFrame): 待处理的Pandas DataFrame。\n",
    "        group_source_col (str): 用于分箱和分组的源列名。此列应该是数值类型，以便计算分位数。\n",
    "        target_col (str): 包含缺失值(NaN)并需要被填充的目标列名。\n",
    "\n",
    "    返回值:\n",
    "        pd.DataFrame: 填充了缺失值后的DataFrame。\n",
    "    \"\"\"\n",
    "        \n",
    "    temp_bin_col = f'{group_source_col}_bin'\n",
    "    df[temp_bin_col] = pd.qcut(df[group_source_col], \n",
    "                        q=[0, 0.25, 0.5, 0.75, 1.0], \n",
    "                        labels=[1,2,3,4], \n",
    "                        duplicates='drop')\n",
    "    df[target_col] = df[target_col].fillna(df.groupby(temp_bin_col)[target_col].transform('median'))\n",
    "    df.drop(columns=[temp_bin_col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "172c4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#认为Social_event_attendance和Time_spend_Alone有关，用前者分箱为后者取中位数缺失值\n",
    "#认为Going_outside和Time_spend_Alone有关，用前者分箱为后者取中位数填充缺失值\n",
    "#再用全部的中位数fillna还没有被补全的Time_spend_alone\n",
    "all_data = fill_missing_by_quantile_group(all_data, 'Social_event_attendance', 'Time_spent_Alone')\n",
    "all_data = fill_missing_by_quantile_group(all_data, 'Going_outside', 'Time_spent_Alone')\n",
    "all_data['Time_spent_Alone'].fillna(all_data['Time_spent_Alone'].median(), inplace=True)\n",
    "\n",
    "all_data = fill_missing_by_quantile_group(all_data, 'Going_outside', 'Social_event_attendance')\n",
    "all_data = fill_missing_by_quantile_group(all_data, 'Friends_circle_size', 'Social_event_attendance')\n",
    "all_data['Social_event_attendance'].fillna(all_data['Social_event_attendance'].median(), inplace=True)\n",
    "\n",
    "all_data = fill_missing_by_quantile_group(all_data, 'Social_event_attendance', 'Going_outside')\n",
    "all_data['Going_outside'].fillna(all_data['Going_outside'].median(), inplace=True)\n",
    "\n",
    "all_data = fill_missing_by_quantile_group(all_data, 'Post_frequency', 'Friends_circle_size')\n",
    "all_data = fill_missing_by_quantile_group(all_data, 'Going_outside', 'Friends_circle_size')\n",
    "all_data['Friends_circle_size'].fillna(all_data['Friends_circle_size'].median(), inplace=True)\n",
    "\n",
    "all_data = fill_missing_by_quantile_group(all_data, 'Friends_circle_size', 'Post_frequency')\n",
    "all_data['Post_frequency'].fillna(all_data['Post_frequency'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40b3297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = all_data.select_dtypes(include=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09d602f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Final Advanced Features ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Creating Final Advanced Features ---\")\n",
    "all_data['social_mean'] = all_data[numerical_features].mean(axis=1)\n",
    "all_data['social_std'] = all_data[numerical_features].std(axis=1)\n",
    "all_data['social_sum'] = all_data[numerical_features].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6895ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans方法获得聚类特征\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "all_data['cluster'] = kmeans.fit_predict(all_data[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05a2eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得多项式特征\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False) # interaction_only=False gives x*y AND x^2\n",
    "# 同时生成多项式特征（如 a^2, b^2）和交互特征（如 a*b）；True只生成交互特征\n",
    "poly_features = poly.fit_transform(all_data[numerical_features])\n",
    "poly_df = pd.DataFrame(poly_features, \n",
    "                        columns=poly.get_feature_names_out(numerical_features))\n",
    "poly_df.drop(columns=numerical_features, inplace=True)\n",
    "all_data = pd.concat([all_data.reset_index(drop=True), poly_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467955f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充分类型缺失值，用Unknown填充\n",
    "all_data.fillna({'Stage_fear': 'Unknown', 'Drained_after_socializing': 'Unknown', 'match_p': 'Unknown'}, inplace=True)\n",
    "all_data = pd.get_dummies(all_data, \n",
    "                            columns=['Stage_fear', 'Drained_after_socializing', 'match_p', 'cluster'], \n",
    "                            prefix=['Stage', 'Drained', 'match', 'cluster'])\n",
    "\n",
    "\n",
    "X = all_data[:len(train_df)]\n",
    "X_test = all_data[len(train_df):]\n",
    "\n",
    "y_encoded = LabelEncoder().fit_transform(y_train_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39a111b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final Ensemble with New Advanced Features ---\n",
      "--- FOLD 1/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n",
      "--- FOLD 2/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n",
      "--- FOLD 3/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n",
      "--- FOLD 4/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n",
      "--- FOLD 5/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n",
      "--- FOLD 6/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n",
      "--- FOLD 7/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n",
      "--- FOLD 8/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n",
      "--- FOLD 9/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n",
      "--- FOLD 10/10 ---\n",
      "  Training xgb...\n",
      "  Training lgb...\n",
      "  Training cat...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training Final Ensemble with New Advanced Features ---\")\n",
    "xgb_params = { 'objective': 'binary:logistic', \n",
    "                'tree_method': 'gpu_hist', \n",
    "                'random_state': 42, \n",
    "                'n_estimators': 1000, \n",
    "                'learning_rate': 0.01, \n",
    "                'max_depth': 7, \n",
    "                'subsample': 0.7, \n",
    "                'colsample_bytree': 0.7 }\n",
    "\n",
    "lgb_params = { 'objective': 'binary', \n",
    "                'metric': 'accuracy', \n",
    "                'n_estimators': 1000, \n",
    "                'random_state': 42, \n",
    "                'verbose': -1, \n",
    "                'learning_rate': 0.025, \n",
    "                'num_leaves': 150 }\n",
    "\n",
    "cat_params = { 'objective': 'Logloss', \n",
    "                'iterations': 1000, \n",
    "                'random_seed': 42, \n",
    "                'verbose': 0, \n",
    "                'learning_rate': 0.0212, \n",
    "                'depth': 10 }\n",
    "\n",
    "models = {'xgb': xgb.XGBClassifier(**xgb_params), \n",
    "            'lgb': LGBMClassifier(**lgb_params), \n",
    "            'cat': CatBoostClassifier(**cat_params)}\n",
    "\n",
    "#折外观测 Out-of-Fold\n",
    "oof_preds = {name: np.zeros(len(X)) for name in models.keys()}\n",
    "test_preds = {name: np.zeros(len(X_test)) for name in models.keys()}\n",
    "\n",
    "N_SPLITS = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y_encoded)):\n",
    "    print(f\"--- FOLD {fold+1}/{N_SPLITS} ---\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "    for name, model in models.items():\n",
    "        print(f\"  Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        oof_preds[name][val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "        test_preds[name] += model.predict_proba(X_test)[:, 1] / N_SPLITS\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e2b5a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stacking and Creating Final Submission ---\n",
      "\n",
      "✅ Final submission file 'submission_final_advanced_feats_v2.csv' created!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Stacking and Creating Final Submission ---\")\n",
    "oof_df = pd.DataFrame(oof_preds); test_preds_df = pd.DataFrame(test_preds)\n",
    "meta_model = LogisticRegression(random_state=42); meta_model.fit(oof_df, y_encoded)\n",
    "final_predictions = meta_model.predict(test_preds_df)\n",
    "le_final = LabelEncoder().fit(['Extrovert', 'Introvert']); final_labels = le_final.inverse_transform(final_predictions)\n",
    "submission_df = pd.DataFrame({'id': test_id, 'Personality': final_labels})\n",
    "submission_df.to_csv('submission_final_advanced_feats_v2.csv', index=False)\n",
    "print(\"\\n✅ Final submission file 'submission_final_advanced_feats_v2.csv' created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9260d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
